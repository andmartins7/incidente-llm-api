# API de ExtraÃ§Ã£o de Incidentes com LLM Local (Ollama)

Este projeto implementa uma **API em Python (FastAPI)** que recebe uma descriÃ§Ã£o textual de um incidente,
utiliza um **LLM local via Ollama** para extrair campos estruturados e retorna um **JSON** com as chaves:

- `data_ocorrencia` â€” data e hora do incidente (se presente)
- `local` â€” local do incidente
- `tipo_incidente` â€” tipo/categoria do incidente
- `impacto` â€” breve descriÃ§Ã£o do impacto

> **Foco**: boas prÃ¡ticas, organizaÃ§Ã£o do cÃ³digo, reprodutibilidade local e integraÃ§Ã£o com LLM **offline** (sem serviÃ§os de nuvem).

---

## ğŸ”§ Requisitos

- Python 3.10+
- [Ollama](https://ollama.com/) em execuÃ§Ã£o local
- (Opcional) Docker e Docker Compose

---

## ğŸ“¦ InstalaÃ§Ã£o (modo local)

1) Crie e ative um ambiente virtual:
```bash
python -m venv .venv
source .venv/bin/activate  # no Windows: .venv\Scripts\activate
```

2) Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

3) Inicie o **Ollama** (em outro terminal):
```bash
ollama serve &
# escolha um modelo pequeno:
ollama pull tinyllama
# ou:
ollama pull llama3.2
```

4) Configure variÃ¡veis de ambiente (opcional â€“ podem ser passadas via `.env`):
```bash
export OLLAMA_HOST=http://localhost:11434
export OLLAMA_MODEL=llama3.2  # ou tinyllama
export LOG_LEVEL=INFO
export TZ=America/Sao_Paulo
```

5) Rode a API:
```bash
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
```

Abra a documentaÃ§Ã£o interativa em: <http://localhost:8000/docs>

---

## â–¶ï¸ Exemplos de uso

### cURL (POST /extract)
```bash
curl -s -X POST http://localhost:8000/extract   -H "Content-Type: application/json"   -d '{{
        "texto": "Ontem Ã s 14h, no escritÃ³rio de SÃ£o Paulo, houve uma falha no servidor principal que afetou o sistema de faturamento por 2 horas."
      }}' | jq .
```

### Python (requests)
```python
import requests
payload = {{
  "texto": "Ontem Ã s 14h, no escritÃ³rio de SÃ£o Paulo, houve uma falha no servidor principal que afetou o sistema de faturamento por 2 horas."
}}
print(requests.post("http://localhost:8000/extract", json=payload).json())
```

SaÃ­da **esperada** (exemplo):
```json
{{
  "data_ocorrencia": "2025-08-12 14:00",
  "local": "SÃ£o Paulo",
  "tipo_incidente": "Falha no servidor",
  "impacto": "Sistema de faturamento indisponÃ­vel por 2 horas"
}}
```

> ObservaÃ§Ã£o: o LLM local pode nÃ£o acertar todos os campos. A API valida o JSON e
> aplica um **fallback por regras** para preencher o que estiver faltando.

---

## ğŸ§  Como funciona

1. **PrÃ©-processamento** do texto (`src/preprocess.py`): normaliza espaÃ§os, corrige padrÃµes comuns de hora/data etc.
2. **Chamada ao LLM local** (`src/llm_client.py`): usa o endpoint **Ollama** (`/api/chat`) com um prompt **instruÃ­do a responder somente JSON**.
3. **ValidaÃ§Ã£o e Fallback** (`src/extractors.py`): valida o JSON com um **schema JSON (jsonschema)**; se estiver invÃ¡lido ou incompleto, usa **expressÃµes regulares**, **dicionÃ¡rio/NER local de localidades** e **heurÃ­sticas** (com `dateparser`) para preencher os campos.
4. **API FastAPI** (`src/main.py`): expÃµe `POST /extract`, `GET /healthz`, `GET /example` e `GET /metrics` (Prometheus). Inclui **logs estruturados** em JSON.

---

## ğŸ§ª Testes

Execute testes bÃ¡sicos (sem LLM, usando fallback):
```bash
pytest -q
```

---

## ğŸ³ Docker (opcional)

### Executar somente a API (supondo Ollama fora do container)
```bash
docker build -t incidente-llm-api -f docker/Dockerfile .
docker run --rm -p 8000:8000   -e OLLAMA_HOST=http://host.docker.internal:11434   -e OLLAMA_MODEL=llama3.2   incidente-llm-api
```

### Docker Compose (API + Ollama)
> **AtenÃ§Ã£o**: baixar o modelo pode levar alguns minutos na primeira vez.
```bash
docker compose -f docker/compose.yml up --build
# API: http://localhost:8000/docs
# Ollama: porta 11434 dentro da rede do compose
```

---

## ğŸ“ Estrutura do projeto

```
incidente_llm_api/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ llm_client.py
â”‚   â””â”€â”€ extractors.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ curl_example.sh
â”‚   â””â”€â”€ install_ollama_linux.sh
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ incidento_exemplo.txt
â””â”€â”€ docker/
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ compose.yml
```

---

## ğŸ“ Notas

- A API Ã© **100% local** e **nÃ£o** utiliza provedores de nuvem.
- Use `LLM` **pequeno** (ex.: `tinyllama`, `llama3.2`) apenas para validar a integraÃ§Ã£o.
- Campos ausentes no texto serÃ£o retornados como string vazia.
- A API injeta no prompt a data/hora atual (fuso `America/Sao_Paulo`) para ajudar o LLM a resolver textos relativos (ex.: *ontem*, *hoje*).

---

## ğŸ§­ Roadmap (sugestÃµes)
- Melhorar o parser de localidades com dicionÃ¡rio/NER local.
- Adicionar schema JSON com `jsonschema` para validaÃ§Ã£o estrita.
- Expor mÃ©tricas (Prometheus) e log estruturado.
